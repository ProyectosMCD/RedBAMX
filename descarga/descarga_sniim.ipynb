{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68yOgpwhj2xJ"
      },
      "source": [
        "# Descarga de datos del SNIIM (Sistema Nacional de información e Integración de Mercados)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ2ao2LRYshN"
      },
      "source": [
        "El **SNIIM** proporciona información de precios al mayoreo del mercado agroalimentario a fin de coadyuvar a la toma de decisiones en materia de comercio, así como también brinda una atención a los usuarios con apego al marco legal aplicable, comprometidos en la mejora continua.\n",
        "\n",
        "No se encontraron los datos disponibles en descarga directa, tampoco a través de APIs, por lo tanto se hace necesario realizar *web scrapping* para la automatización y descarga de los datos.\n",
        "\n",
        "Por ahora sólo se realiza la descarga para la categoría de **\"Frutas y Hortalizas\"**, las cuales son las de interés en este momento para la **Red de Banco de Alimentos (RedBAMx).**\n",
        "\n",
        "Basado en el scrapper hecho por México Abierto:\n",
        "https://github.com/mxabierto/scraper-sniim/blob/master/sniim/precios_historicos.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTwMTbU8j2xO"
      },
      "source": [
        "#### Importar librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XjuACWQ0DRcL"
      },
      "outputs": [],
      "source": [
        "# Importar librerías\n",
        "\n",
        "import re\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "#from urllib.error import HTTPError\n",
        "\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import logging"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se configura módulo logging para crear log de descargas, según documentación https://docs.python.org/es/3/howto/logging.html"
      ],
      "metadata": {
        "id": "9hIR8QVWu90R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#logger = logging.getLogger('my_logger')\n",
        "logging.basicConfig(\n",
        "    filename='./descarga.log',\n",
        "    filemode='a',\n",
        "    encoding='utf-8',\n",
        "    format='%(asctime)s, %(levelname)s %(message)s',\n",
        "    datefmt='%d/%m/%Y %I:%M:%S %p',\n",
        "    level=logging.DEBUG,\n",
        "    force=True # Resets any previous configuration\n",
        ")\n",
        "\n",
        "#logging.debug('This message should go to the log file')\n",
        "#logging.info('So should this')\n",
        "#logging.warning('And this, too')\n",
        "#logging.error('And non-ASCII stuff, too, like Øresund and Malmö')"
      ],
      "metadata": {
        "id": "1gvA4zWBouda"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne3Yyf83j2xQ"
      },
      "source": [
        "#### Creación de carpetas temporales y de salida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SzgvDiVcFi94"
      },
      "outputs": [],
      "source": [
        "# Creación de carpetas temporales y de salida\n",
        "#print(os.getcwd())\n",
        "subdir1 = './temp'\n",
        "subdir2 = './raw_data'\n",
        "\n",
        "try:\n",
        "  if not os.path.exists(subdir1):\n",
        "    os.makedirs(subdir1)\n",
        "  if not os.path.exists(subdir2):\n",
        "    os.makedirs(subdir2)\n",
        "except Exception as e:\n",
        "        print (\"Error al crear carpeta: \", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOCdOomHj2xQ"
      },
      "source": [
        "#### Función que a partir de la URL hace el scrapping para extraer una tabla de información y guardarla en archivo CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JyEL5aCz0NFt"
      },
      "outputs": [],
      "source": [
        "# Función que realiza el scrapping para extraer una tabla de información y  guardarla en archivo CSV\n",
        "def creaTabla(url, archivo_salida, paginacion, categoria):\n",
        "    print (url, archivo_salida, paginacion, categoria)\n",
        "    try:\n",
        "\n",
        "        with urllib.request.urlopen(url+str(paginacion), timeout=500) as response, \\\n",
        "          open('temp/'+archivo_salida, 'wb') as out_file, \\\n",
        "          open('raw_data/'+archivo_salida+\".csv\", 'w', newline=\"\\n\") as csvfile:\n",
        "\n",
        "            data = response.read().decode('utf-8').encode('utf-8')\n",
        "            soup = BeautifulSoup(data)\n",
        "\n",
        "\n",
        "            try:\n",
        "              paginas = soup.find('span', attrs={\"id\":\"lblPaginacion\"}).get_text()\n",
        "              print(paginas, paginas != 'Página  1 de  1')\n",
        "\n",
        "              total_paginas = paginas.split(' ')[-1]\n",
        "\n",
        "              if total_paginas != '1':\n",
        "                  return creaTabla(url, archivo_salida, paginacion * int(total_paginas), categoria)\n",
        "              out_file.write(data)\n",
        "\n",
        "\n",
        "              table = soup.find('table',attrs={\"id\":\"tblResultados\"})\n",
        "\n",
        "              spamwriter = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "\n",
        "              for row in table.find_all(\"tr\"):\n",
        "                  x = [td.get_text() for td in row.find_all(\"td\", class_=lambda x: x != 'encabACT2')] #ignorar row de categoría\n",
        "                  #x.append(categoria)\n",
        "                  spamwriter.writerow(x)\n",
        "\n",
        "\n",
        "            except:\n",
        "              print ('La página no tiene tabla, ', url, paginacion)\n",
        "              #borrar archivos\n",
        "              if os.path.exists(\"raw_data/\"+archivo_salida+\".csv\"):\n",
        "                csvfile.close()\n",
        "                os.remove(\"raw_data/\"+archivo_salida+\".csv\")\n",
        "              if os.path.exists(\"temp/\"+archivo_salida):\n",
        "                out_file.close()\n",
        "                os.remove(\"temp/\"+archivo_salida)\n",
        "              pass\n",
        "    except Exception as e:\n",
        "      print (\"Error: \", url,e)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Automatización de la descarga de datos por intervalo de fechas"
      ],
      "metadata": {
        "id": "Uwy19hMnl6_C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSlrJGXZcloU",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "logging.info('Descarga de datos SNIIM iniciada.')\n",
        "\n",
        "# Se definen las fechas de búsqueda\n",
        "str_fecha_inicio = \"01/01/2020\"\n",
        "str_fecha_final = \"30/12/2023\"\n",
        "max_intervalo_años = 5\n",
        "\n",
        "# Se convierten a tipo date\n",
        "dt_fecha_inicio = datetime.strptime(str_fecha_inicio, '%d/%m/%Y')\n",
        "dt_fecha_final = datetime.strptime(str_fecha_final, '%d/%m/%Y')\n",
        "\n",
        "# Ligas a precios\n",
        "mapa_precios_url = \"http://www.economia-sniim.gob.mx/mapa.asp\"\n",
        "base_url = 'http://www.economia-sniim.gob.mx/Nuevo/Consultas/MercadosNacionales/PreciosDeMercado/Agricolas'\n",
        "frutas_hortalizas_endpoint = \"/ResultadosConsultaFechaFrutasYHortalizas.aspx\"\n",
        "\n",
        "\n",
        "with urllib.request.urlopen(mapa_precios_url) as response, \\\n",
        "  open('mapa.aspx', 'wb') as out_file:\n",
        "    data = response.read()#.decode('utf-8').encode('utf-8') # a `bytes` object\n",
        "    out_file.write(data)\n",
        "    soup_directorio = BeautifulSoup(data)\n",
        "\n",
        "    for anchor in soup_directorio.findAll('a', string=re.compile(\"Precio\\sde\\s[^la|los|Granos]\\w+\")):\n",
        "        producto_id = re.search(r\"ProductoId=(\\d+)\", anchor['href']).group(1)\n",
        "        nombre_de_lista = anchor.string\n",
        "\n",
        "        # Si el nombre de lista contiene \"/\" se reemplaza por \"_\" para prevenir error al crear archivos con este nombre\n",
        "        if \"/\" in nombre_de_lista:\n",
        "            nombre_de_lista = nombre_de_lista.replace(\"/\", \"_\")\n",
        "\n",
        "        '''\n",
        "        # Prueba descargando por producto\n",
        "        if nombre_de_lista == \"Precio de Limón Real\" :\n",
        "          flag = True\n",
        "        else:\n",
        "          flag = False\n",
        "        if flag:\n",
        "        '''\n",
        "\n",
        "        # Si el periodo de tiempo es mayor a 5 años, se descarga por segmentos de 5 años para prevenir error en el servidor de datos\n",
        "        if(dt_fecha_final.year - dt_fecha_inicio.year) > max_intervalo_años:\n",
        "          for anio_inicio in range(dt_fecha_inicio.year, dt_fecha_final.year, max_intervalo_años):\n",
        "            # Si el año es mayor que el año de la fecha final, se toma el de la fecha final\n",
        "            if(anio_inicio + (max_intervalo_años-1)) > dt_fecha_final.year:\n",
        "              anio_final = dt_fecha_final.year\n",
        "            else:\n",
        "              anio_final = anio_inicio + (max_intervalo_años-1)\n",
        "\n",
        "            url = base_url + frutas_hortalizas_endpoint + f\"?ProductoId={producto_id}&fechaInicio=01/01/{anio_inicio}&fechaFinal=31/12/{anio_final}&RegistrosPorPagina=\"\n",
        "            #print(url)\n",
        "\n",
        "            if(url.find(\"http:\") >= 0):\n",
        "              creaTabla(url, nombre_de_lista.replace(\" \",\"_\")+\"_\"+str(anio_inicio)+\"-\"+str(anio_final), 1000, nombre_de_lista.replace(\" \",\"_\"))\n",
        "\n",
        "\n",
        "        # De lo contrario, si el periodo de tiempo es menor a 5 años, se descarga en un solo segmento\n",
        "        else:\n",
        "            url = base_url + frutas_hortalizas_endpoint + f\"?ProductoId={producto_id}&fechaInicio={str_fecha_inicio}&fechaFinal={str_fecha_final}&RegistrosPorPagina=\"\n",
        "            #print(url)\n",
        "\n",
        "            if(url.find(\"http:\") >= 0):\n",
        "              creaTabla(url, nombre_de_lista.replace(\" \",\"_\")+\"_\"+str(dt_fecha_inicio.year)+\"-\"+str(dt_fecha_final.year), 1000, nombre_de_lista.replace(\" \",\"_\"))\n",
        "\n",
        "print(\"Descarga finalizada.\")\n",
        "logging.info('Descarga de datos SNIIM finalizada.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descarga de archivos .zip\n",
        "#!zip -r /content/raw_data.zip /content/raw_data\n",
        "#from google.colab import files\n",
        "#files.download(\"/content/raw_data.zip\")"
      ],
      "metadata": {
        "id": "egaHwqli8f-I"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}